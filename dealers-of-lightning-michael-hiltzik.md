The Alto aimed to be not a machine of its time, but of the future. Computer memory was horrifically expensive at the moment, true, but it was getting cheaper every week. At the rate prices were falling, the same memory that cost ten grand in 1973 would be available in 1983 for thirty dollars. The governing principle of PARC was that the place existed to give their employer that ten-year head start on the future. They even contrived a shorthand phrase to explain the concept. The Alto, they said, was a time machine.

---

One was Xerox’s money, a seemingly limitless cascade of cash flowing from its near-monopoly on the office copier. The second was a buyer’s market for high-caliber research talent. With the expenses and politics of the Vietnam War cutting into the government’s research budget and a nationwide recession exerting the same effect on corporate research, Xerox was one of the rare enterprises in a position to bid for the best scientists and engineers around.

---

The final factor was management. PARC was founded by men whose experience had taught them that the only way to get the best research was to hire the best researchers they could find and leave them unburdened by directives, instructions, or deadlines.

---

“The notion of a human being having to punch holes in lots of cards, keep these cards straight, and then take this deck of what might be hundreds and hundreds of cards to a computer… You come back the next day and find out that your program executed up until card 433 and then stopped because you left out a comma. You fix that and this time the program gets to card 4006 and stops because you forgot to punch an O instead of a zero or some other stupid reason. It was bleak.” Taylor perceived the need for something entirely new. “I started talking functionally,” he said. He asked himself: Which organ provides the greatest bandwidth in terms of its access to the human brain? Obviously, the eyeball. If one then contemplated how the computer could best communicate with its human operator, the answer suggested itself. “I thought the machine should concentrate its resources on the display.”

---

By 1962, however, when Licklider was tapped to run a new Information Processing Techniques Office, or IPTO, the urgency had waned. Whatever military orientation ARPA still harbored was visible only as a sort of artifact, as when Licklider discovered by accident that one “cloak-and-dagger” project under his nominal jurisdiction was so highly classified even he was not cleared to know what it was. (“That made me nervous,” he admitted later.) 

ARPA had refocused itself on civilian research in broad scientific areas, some of them having only tenuous relevance to national security. “I did not feel much pressure to make a military case for anything,” Licklider told an interviewer years later. Of course, the Pentagon did expect that the agency’s work might serendipitously lead to. solutions of some of its technical problems, such as the vexing issue of “command and control”: how to employ effectively the immense volume of information generated on a battlefield to manage the armed forces’ increasingly elaborate weapons systems. 

For years the military had viewed this issue in terms of training the human beings with their fingers on the triggers. Licklider informed his bosses that the real solution lay in making the machine meet the human halfway. This was something he called “man-computer symbiosis,” a subject on which he happened to have published a paper two years earlier. Traditionally, problems had to be written and presented to the computer very carefully so the machine would understand every step. One tiny error, and all computation would cease. Try planning a battle under these conditions—you would be obliterated before reaching the second step in the process. But what if the system were designed so the computer was no longer a mute data manipulator, but a participant in a dialogue—something, he had written in that paper, like “a colleague whose competence supplements your own?”

---

Ivan Sutherland was a brilliant MIT graduate who happened to be serving with the Army as a first lieutenant. Only twenty-six, he had already amassed an enviable research record, the crowning achievement of which had been the development of the first interactive computer graphics program. Known as Sketchpad, the system allowed a user to draw highly detailed and complex drawings directly on a computer’s cathode-ray screen using a light pen and store them in memory.

---

Nothing of the kind ever occurred at Xerox research. The Webster engineering and research staff treated the new science of computer-aided design with utter indifference. Webster’s classically educated chemists, physicists, and metallurgists devoted their attention to narrow, product-oriented tasks, trying to develop better toners and photoreceptors to drop into copiers designed the same old way. As far as that went they were talented enough, but they had no incentive to keep up with new research techniques. As for applying imagination to an entirely new science, concept, or machine, Webster was hopeless. It was not research as Jack Goldman understood the word; it was product development, which was something very different.

---

“Real research people tend to interact with the world at large,” he observed. “They know what’s happening on the university campuses and get invited back and forth, so they become an avenue through which you can attract new ideas into the company. Research is a funnel through which you can bring in people who normally won’t talk to the guys down in the trenches designing equipment,” he said. Xerox had not been getting new ideas, and it showed.

---

The architecture of information: The phrase might as well have been chiseled over the doors of the hilltop palace Xerox would soon build to house a group of employees quite unlike any others the company had ever placed on its payroll. “It was a great phrase,” one PARC engineer said later, “because nobody knew exactly what it meant. So there were quite a few interesting things you could do and simply cite that as the justification.”

---

Pake hesitated, wondering about Xerox’s resolve over the long haul. “I had a lot of friends at other industrial research establishments and the usual thing they were worried about was the feast-or-famine effect,” he said later. “You know, in the good business years the company invests in research, but in the bad years they want to pull out.” That was a recipe for wasting millions of dollars. “Research is a steady-state thing. You can’t just turn it on and off.”

---

“I said if you hire me you will get nothing of business value in five years,” he recalled. “But if you don’t have something of value in ten years, then you’ll know you’ve hired the wrong guy.”

---

Pake directed Goldman’s attention westward.

---

Jones, Squires, and Gloria Warner, a senior secretary who relocated from Webster to work for Pake, spent the next week working like char-women. With brooms, buckets, and mops purchased from the nearest K Mart, they swept up the accumulated filth themselves and installed a rickety table and chairs in the clearing.

---

Taylor’s capacious net would come to embrace areas of computer research that barely existed when ARPA delivered its lifesaving shot to Wes Clark’s project. At ARPA he funded the country’s first full-fledged graduate degree programs in computer science at Stanford, Carnegie-Mellon, and MIT. Some fields of study virtually owed their existence to his largesse. Among them was computer graphics, which came to life at the University of Utah when Dave Evans, a devout Mormon who had led the Genie team building the time-sharing SDS 940 at UC Berkeley, called Taylor to say his alma mater had invited him to return to Salt Lake to start a computer program. How about an ARPA project, he asked, to get it going? Computer graphics was then attracting almost no one’s attention, for the simple reason that most computers lacked visual displays of any kind. If Evans was willing to start such a program in the backwater of Utah, where it could develop in pristine isolation from the traditionalist thinking elsewhere, Taylor was all for it. The venture turned out better than anyone could have expected. The program Taylor funded partially as a personal experiment and partially as a favor to an old friend evolved into a world leader in computer graphics research. His most enduring legacy, however, was not a university program but a leap of intuition that tied together everything else he had done. This was the ARPANET, the precursor of today’s Internet.

---

Whether they were at MIT, Stanford, or UCLA, researchers were all looking for answers to the same general questions. “These people began to know one another, share a lot of information, and ask of one another, ‘How do I use this? Where do I find that?’” Taylor recalled. “It was really phenomenal to see this computer become a medium that stimulated the formation of a human community.” There was still a long way to go before reaching that ideal, however. The community was less like a nation than a swarm of tribal hamlets, often mutually unintelligible or even mutually hostile. Design differences among their machines kept many groups digitally isolated from the others. The risk was that each institution would develop its own unique and insular culture, like related species of birds evolving independently on islands in a vast uncharted sea. Pondering how to bind them into a larger whole, Taylor sought a way for all groups to interact via their computers, each island community enjoying constant access to the others’ machines as though they all lived on one contiguous virtual continent.

---

The notion that any arm of the Pentagon could engage in wholly innocent and purely civilian research incited mistrust across the country. As a defensive measure, ARPA started to shed its civilian entanglements and consciously remake itself into what the nation thought it was anyway—an arm of the war machine. When the Caltech engineer Eberhardt Rechtin succeeded Herzfeld as director in 1967, he assured his congressional overseers he would nudge ARPA toward “mission-oriented” objectives—programs aimed at satisfying chiefly military goals. The 1969 Mansfield Amendment to the military appropriations bill would formalize the trend, directing the Pentagon henceforth to fund only projects of obvious military relevance. As if to underscore the point, the amendment changed ARPA’s name to DARPA, the Defense Advanced Research Projects Agency.

---

Taylor beheld the emasculation of government research with frank alarm. “Most of the time I was there ARPA was going for projects with an order-of-magnitude impact on the state of the science,” he reflected. “We had made a decision that we would not go for incremental things. But as soon as you get mission orientation you’re focusing on very narrow objectives.”

---

In Pake’s hard science universe, where researchers laid their bricks upon foundations that had been built as long as three centuries earlier, a doctorate was a certificate of genuine originality and achievement. That was not true in the fledgling science of computing, which was erecting its own academic foundation as it went along. Nor did Pake’s viewpoint apply very well to Taylor’s unique abilities as a master motivator of top research talent, which could never be encompassed within the rubric of any advanced university degree.

---

All he asked to be spelled out was Pake’s understanding that Xerox’s cherished “office of the future” would embrace networking and interactive computers. Pake agreed without devoting much thought to what those terms implied.

---

Pake also established by the close of 1970 a full research agenda. In a corporate memo dated January 4, 1971, he outlined an ambitious program for his group of twenty-three, augmented by another eight or ten professionals due to start work over the following few months. The Systems Science Lab was to take over development of a laser-driven computer printer whose inventor, a Webster engineer, had come west after failing to interest his bosses in its potential. SSL researchers would also investigate optical memories, a technology that would eventually give rise to today’s compact disc and CD-ROM, and speech recognition by computer. Taylor’s Computer Science Lab was to pursue his pet interest in graphics while developing specifications for a basic center-wide computer system. And GSL was assigned studies in solid-state technologies, including the electrical and optical qualities of crystals.

---

With scant equipment of any value on the premises, the building stayed unlocked and hospitable to outsiders. “We were physically adjacent to Stanford University, so there were visitors dropping in and out of the lab all the time. A lot of us even came to feel we were sort of like university instructors who got to spend all our time doing research without having to teach classes. So we operated as though this were an open environment where we were free to share what we were doing with anyone we wanted to.”

---

In a superb buyers’ market for research and engineering talent, PARC’s lavish budget and open-ended charter stood alone among corporate entities. Other industrial research centers might enjoy generous funding or comparably liberal charters, but none had both the open checkbook and apparent immunity from product development pressures enjoyed by PARC.

---

He was right about that. Toward the end of 1970 Taylor called in some of his old chits to stage a pair of dazzling heists. The first was a raid on the only laboratory on the West Coast—possibly the country—whose work on interactive computing met his stern standards. The lab belonged to the legendary engineer Douglas C. Engelbart, an adamantine visionary who held court out of a small think tank called SRI, or the Stanford Research Institute, a couple of miles north of Palo Alto in the community of Menlo Park. There Engelbart had established his “Augmentation Research Center.” The name derived from his conviction that the computer was not only capable of assisting the human thought process, but reinventing it on a higher plane. The “augmentation of human intellect,” as he defined it, meant that the computer’s ability to store, classify, and retrieve information would someday alter the very way people thought, wrote, and figured.

---

His essay dealt chiefly with technology’s ability to manage information. Bush discerned the birth of what would come to be called the “information glut” and projected it forward to a cacophonous posterity. “Publication has been extended far beyond our present ability to make real use of the record,” he wrote. “The summation of human experience is being expanded at a prodigious rate, and the means we use for threading through the consequent maze to the momentarily important item is the same as was used in the days of square-rigged ships.” Himself the inventor of a successful analog computer, Bush understood that computer technology might help society draw sense out of the chaos. He sketched out something called the “memex,” which he described as “a device in which an individual stores all his books, records, and communications, and which is mechanized so that it may be consulted with exceeding speed and flexibility.” The mechanism of consultation would be “associative indexing… whereby any item may be caused at will to select immediately and automatically another. This is the essential feature of the memex.”

---

Engelbart linked video terminals to mainframes by cable and communicated with the machines via televised images. To allow the user to move the insertion point, or cursor, from place to place in a block of text instantaneously, he outfitted a hollowed-out block of wood with two small wheels fixed at right angles so it could be rolled smoothly over a flat surface. The wheels communicated their motion to potentiometers whose signals in turn were translated by the computer into the placement of the cursor on the screen. From this crude device would spring an entire culture. “No one is quite sure why it got named a ‘mouse,’” Engelbart said years later. “None of us thought that the name would have stayed with it, out in the world.” The entire interactive system—mouse, screen, computer, software, and underlying philosophy—was known by the acronym “NLS,” for “oNLine System.”

---

Until 1968 Engelbart and his aides labored in relative obscurity, their work known only within the insular fraternity of government grant-makers and computer theorists. That year he requested ninety minutes to demonstrate NLS at the Fall Joint Computer Conference of two leading engineering societies, scheduled for San Francisco in December. The result was one of the most famous events in computing history. The mouse, making its first public appearance, was the least of it. Engelbart and his sixteen assistants stretched existing electronic technology nearly to the breaking point. He recalled later: “We built special electronics that picked up the control inputs from my mouse, keyset, and keyboard and piped them down to SRI [that is, Stanford Research Institute] over a telephone hookup. We leased two microwave lines up from our laboratory, roughly thirty miles. It took two additional antennas on the roof at SRI, four more on a truck on Skyline Boulevard, and two on the roof of the conference center. It cost money… The nice people at ARPA and NASA, who were funding us, effectively had to say, ‘Don’t tell me!’” The effort was worth every penny. The audience was riveted, as Engelbart in his subdued drone described and demonstrated a fully operational system of interactive video conferencing, multimedia displays, and split-screen technology. At one point half of a twenty-foot-tall projection screen was occupied by a live video image of Engelbart on stage, the other half by text transmitted live from Menlo Park (it was a shopping list including apples, oranges, bean soup, and French bread). Minutes later the screen carried a live video image of a hand rolling the unusual “mouse” around a desktop while a superimposed computer display showed how the cursor simultaneously and obediently followed its path.

---

Engelbart’s self-defined mission was not to produce a product, or even a prototype; it was an open-ended search for knowledge. Consequently, no project in his lab ever seemed to come to an end. Whenever one approached a milestone he would abruptly redefine it, condemning the lab to months or years of further work. The finish line was constantly receding, like the oasis in a desert mirage. Said one long-term member of his lab: “We were like rats running in his maze.”

---

Bill English had been the invisible guiding hand behind the 1968 demo. He was ferociously loyal to his boss but bridled at the lab’s perpetual lack of closure. Some also believed he was fed up with Engelbart’s way of monopolizing credit for the lab’s accomplishments. Taylor offered English a solution to both complaints: reproduce NLS, or something like it, at PARC. English could thus fulfill his treasured goal of bringing the system to commercial fruition and be in charge of his own lab, out from under the shadow of the implacable Engelbart. Whether English hesitated leaving the leader he had followed for nearly a decade is hard to say, but he continued the raid where Taylor left off, eventually recruiting a dozen of Engelbart’s most important followers.

---

The BCC pioneers were about to become victims of the “second-system effect.” The theory of second systems was formulated by an IBM executive named Frederick Brooks, whose career supervising large-scale software teams taught him that designers of computer systems tend to build into their second projects all the pet features that tight finances or short deadlines forced them to leave out of their first. The result is an overgrown, inefficient monstrosity that rarely works as expected. As he put it in his pithy masterpiece, The Mythical Man-Month: “The second is the most dangerous system a man ever designs.” The BCC machine could have sprung full-blown from the pages of Brooks’s text. As Lampson recalled, the designers of the economical and practical SDS 940 regarded their next machine as an opportunity to “look at all the things you could make much more wonderful, and plan to make them all more wonderful by creating a system that could handle a lot more users and much larger programs and was much faster and used computing resources much more efficiently and was better and more wonderful in every possible way. “It was not a very realistic enterprise,” he acknowledged. “But at the time it seemed great, the proper next step, as second systems often do.”

---

Some of the workers, including Thacker, could tell early on that the project was getting out of hand. The engineer’s engineer possessed the unique trait of aiming for less, not more, in his systems. “This was so unusual for an engineer,” recalled Charles Simonyi

---

Just as BCC filed for bankruptcy (one final drunken party on Friday, November 13, 1970, drained the last of its petty cash), PARC hired six of its best people—Lampson, Thacker, Deutsch, Fiala, a hardware designer named Richard Shoup, and a software programmer named Jim Mitchell.

---

Taylor had completed his team—almost. He had the best hardware man (Thacker), the best designer of operating systems (Lampson), and an entire cell of other computer science prodigies.

---

The BCC group, deeply rooted in the culture of time-sharing, was still intent on getting as many users hooked into a single machine as technologically possible. That goal, as Wes Clark contended, remained incompatible with giving the individual the kind of speed and responsiveness that interactivity required.

---

But one man was way ahead of them all. That one had written a doctoral thesis at Utah in 1969 describing an idealized interactive computer called the FLEX machine. He had experimented with powerful displays and with computers networked in intricate configurations. On page after page of his dissertation he lamented the inability of the world’s existing hardware to realize his dream of an interactive personal computer. He set before science the challenge to build the machine he imagined, one with “enough power to outrace your senses of sight and hearing, enough capacity to store thousands of pages, poems, letters, recipes, records, drawings, animations, musical scores, and anything else you would like to remember and change.” To Taylor he was a soulmate and a profound thinker, capable of seeing a computing future far beyond anything even he could imagine. Among the computer scientists familiar with his ideas, half thought he was a crackpot and the other half a visionary. His name was Alan Kay.

---

Ideaspace Central today is divided between two Southern California locations about ten miles apart. One is Kay’s home in an affluent part of Los Angeles. It is unassuming from the outside except for a towering V-roofed addition. This curious annex was custom-built to shelter a two-story pipe organ professionally hand-crafted of exquisite blond spruce, on which Kay can be heard almost any morning practicing his favorite music by Buxtehude and J. S. Bach. (“ Alan believed his role was to make it possible to build the organ, after which he would be the happy caretaker,” remarked its architect, Greg Harrold.) The second location is a warehouse-like building in Glendale, a smoggy precinct of the San Gabriel Valley just north of L.A. Artfully arranged partitions and bookcases provide Kay with a spacious work area open to the floor through a doorless passage on one side—not too private, for he likes to spend the workday in constant stir, eliciting and dispensing ideas among his co-workers with equal generosity. He greets you wearing an oval name tag reading “Alan” and bearing a picture of Mickey Mouse. It should look ludicrous and it does, until you remember that this is the man whose playful digitized image of Cookie Monster launched the age of the personal computer. Or that he is now employed—as are two other members of the extraordinary team he assembled at PARC—by the Walt Disney Company, which has entrusted him with helping to develop new ways to transmit story and idea from creator to audience.

---

“Alan had been thrown out of every university in the country,” recalled John Warnock,

---

This notion of technology as a means to an end still distinguishes Kay from most other practitioners of the art and science of technology. One factor in his powerful kinship with Bob Taylor was their shared curiosity about what this machine could be made to do, more than how. Notwithstanding his incessant harangues, most of the inspired engineers Taylor recruited to CSL, the Lampsons and Thackers, started out too blindly focused on the issue of what was within their power to actually build. They would ask: What is the next stop on the road? Kay turned the question inside out: Let’s figure out where we want to go, and that will show us how to get there. He never lost sight of the computer’s appropriate station in the world: to conform to the user’s desires, not the other way around.

---

“People get trapped in thinking that anything in the environment is to be taken as a given. It’s part of the way our nervous system works. But it’s dangerous to take it as a given because then it controls you, rather than the other way around. That’s McLuhan’s insight, one of the bigger ones in the twentieth century. Zen in the twentieth century is about taking things that have been rendered invisible by this process and trying to make them visible again.

---

“People get trapped in thinking that anything in the environment is to be taken as a given. It’s part of the way our nervous system works. But it’s dangerous to take it as a given because then it controls you, rather than the other way around. That’s McLuhan’s insight, one of the bigger ones in the twentieth century. Zen in the twentieth century is about taking things that have been rendered invisible by this process and trying to make them visible again.

---

“Parents ask me what they should do to help their kids with science. I say, on a walk always take a magnifying glass along. Be a miniature exploratorium….”

---

Jeffers was a junior, a year behind Kay (although since Kay’s illness lost him a year of school, they graduated together). He was also a superb pianist with perfect pitch and a thriving jazz band. Kay joined up on guitar. The band played Dixieland jazz from Jeffers’s effortless arrangements, an interesting choice if one is looking for a form that imposes strict formal rules on players who are encouraged to break them according to another set of strict formal rules.

---

The title read, “Sketchpad: A Man-Machine Graphical Communications System.” The 1963 MIT doctoral thesis of Ivan Sutherland, Taylor’s predecessor at IPTO, the paper described a program that had become the cornerstone of the young science of interactive computer graphics. Sketchpad worked on only one machine in the world, Wes Clark’s TX-2 at Lincoln Lab. But its precepts were infinitely applicable to a whole range of increasingly nimble and powerful computers then coming into existence. Sketchpad was also, by Evans’s mandate, the cardinal introduction to computing in his doctoral program. “Basically,” Kay said, “you had to understand that before you were a real person at Utah.” Sutherland’s system could create graphic objects of dazzling complexity, all the more amazing given the severe limitations of the contemporary hardware. With Sketchpad the user could skew straight lines into curves (“ rubber-banding”), make engineering-precise lines and angles (the system straightened out the draftsman’s rough sketches), and zoom the display resolution in and out. The program pioneered the “virtual desktop,” in which the user sketched on the visible portion of a theoretical drawing space about one-third of a mile square (the invisible portions were held in the computer’s memory and could be scrolled into view). Contemplating the power of Sketchpad was “like seeing a glimpse of heaven,” Kay said later. “It had all of the kinds of things that the computer seemed to promise. You could think of it as a light that was sort of showing us the way.”

---

“The best outputs that time-sharing can provide are crude green-tinted line drawings and square-wave musical tones. Children, however, are used to finger paints, color television and stereophonic records, and they usually find the things that can be accomplished with a low-capacity time-sharing system insufficiently stimulating to maintain their interest.” Or as Kay and his colleague Adele Goldberg wrote later: “If ‘the medium is the message,’ then the message of low-bandwidth time-sharing is ‘blah.’”

---

They could accept the Sigma, which meant suffering with an inadequate machine and knuckling under to the corporate suits the very first time their interests conflicted. Or they could stand their ground and browbeat Xerox into buying them the PDP. “But that would cost us so many brownie points we figured it was not a good idea,” Lampson recalled. Not a few times in the heat of discussion was the suggestion heard that they should all just quit. Yet a different impulse, one much more powerful, was beginning to assert itself. This was the engineer’s equivalent to the “fight or flight” syndrome, the instinct when confronted by an obstacle not to back off, but to barrel through it. “We started having these long discussions,” Alan Kay remembered. “I was saying, ‘Let’s not chuck our badges in just yet. Let’s think it through.’” Someone else suggested that perhaps they were looking at the question from the wrong end. They had made an issue out of hardware—what machine to buy—when their real concern was software—what programs they could run. What if they simply built their own machine to run the programs they needed? Asking the question was the same as answering it. “The talk,” Kay said, “turned to how long it would take us to build our own PDP—10.” The answer was about one year and less than $ 1 million. The truth was that Xerox had only forbidden the lab to buy a PDP—10. Nobody had said anything about cloning one. “We were fearless,” Lampson recalled. “We had built this BCC machine which was a substantially more complicated and elaborate machine than a PDP—10. And furthermore the underlying technology was evolving very rapidly, so we actually had much better physical resources at our disposal. We had great confidence that we could build this thing with a fairly modest investment of effort, and very little risk. Which turned out to be absolutely correct.” The machine, he said, “was not built because Xerox refused to let us buy a PDP—10, but because we thought it was the path of least resistance.”

---

Work on the clone began in February. Thacker assumed the role of project manager as well as the bedrock task of designing the internal logic. The other jobs got apportioned out to the lab members willy-nilly, as among townspeople at a community barn-raising, often with little relation to the work in which they were most expert—but then, anyone who could make it through Taylor’s selection process was assumed to be smart enough to learn anything. Lampson was an operating systems designer nonpareil, for example, but with Tenex they already had their operating system. So he was dragooned into designing the central processor, a hardware task unlike anything he had ever undertaken in his life.

---

The idea that they were building their own computing environment galvanized them into working at breakneck speed, like settlers hastening to erect a rudimentary shelter before the onset of a hard winter. An almost alchemical change overtook the lab, infusing it with the pure excitement of discovery that a research manager may be lucky to witness once in a lifetime. People would contend later that being forced to clone the PDP—10 was the best thing that ever happened to CSL—even Strassmann eventually bragged, “I made everyone a hero.” This is the period they were referring to, when a bunch of disparate talents came together in appreciation of the virtues of working together on challenges that had virtually no precedent and the air filled with new ideas. Then there was the added satisfaction of testing their mastery of a new technology. “It was fun,” Lampson said later, “to see how easy it was.” The fundamental problem was that to the extent the PDP—10’ s physical design belonged to Digital Equipment Corporation it could not simply be copied, at least not legally. In any case, no one at PARC had spent enough time inside that machine to reproduce it circuit for circuit. The trick was to turn this limitation into an advantage. Since the task at hand was not really to build a PDP—10, only a machine that would follow the same instructions as a PDP—10, there was no reason not to find a shortcut. The best shortcut, as they all knew, was to substitute their own microcode for the PDP—10’ s wiring.

---

Liberated from slavish adherence to the PDP—10 design, they were able to get dozens of functions running faster or cheaper. McCreight performed one such feat with his disk controller. Conventional disk controllers were generally equipped with their own separate processing units, like Stegosauruses with their second brains, which added significantly to their cost and complexity. Poring over the system schematics in his office one day, he was struck by the realization that there would occur certain periods when, having executed one instruction and not yet received the next, MAXC’s central processor would be idle but available, like a car left running unattended in the driveway. “I learned enough about the processor to realize I could use some of those spare cycles,” he recounted. “In effect I could kidnap the processor to do some arithmetic for the disk controller. I wouldn’t have to put so many gates into the disk controller”—saving another few thousand dollars in hardware—” if I could periodically borrow the processor to compute some of the things I needed to compute.” McCreight’s realization was their first embrace of the concept of “multitasking”—giving the processor numerous jobs to juggle at once. Implemented on this modest scale in MAXC, it was destined to pay enormous dividends later.

---

At first glance, devoting so much time and money to reproducing a computer available on the open market seemed sheer profligacy. But from Taylor’s point of view, the assignment to produce a real machine had given his engineers a unique opportunity to parse out their own strengths and weaknesses in ways Taylor could never have devised himself. What emerged at the end of the program was a seamless, remarkably powerful unit.

---

“In a small group the dynamics are like those on a good basketball team,” Kay observed. “Everybody has to be able to play the whole game. Each person should have certain things they’re better at than the others, but everyone should be pretty good at everything.”

---

But PARC had come face to face with a force of nature, the corporate instinct for self-preservation. While Kay urged upon Xerox the virtues of patience and trust in scientific serendipity, Pendery pressed for a definition of its vision that could be reduced to paper and presented in a boardroom. Finally he got it. In mid-1971 George Pake sent up to headquarters a half-inch-thick folder containing seven documents, each written by an individual PARC scientist—scarcely sixty pages altogether. Someone had cheekily labeled it “PARC Papers for Pendery and Planning Purposes.” In lab shorthand they were henceforth known as the “Pendery Papers.” Not since Vannevar Bush had forecast how we might think in his essay for The Atlantic had such a comprehensive vision of technology and the future been set down in writing. The Pendery Papers were at once a survey of the most promising technologies on the horizon and a road map for PARC’s ten-year exploratory journey. Some of the forecasts overshot their marks. Kay, for instance, anticipated (perhaps wishfully) portable flat-screen displays at nominal cost by 1980. Jim Mitchell, writing on future office systems, envisioned error-free and infinitely customizable software, transmitted from vender to buyer over network connections, running flawlessly on a full spectrum of incompatible machines (as of this writing still a hazy dream). But on the whole the package stands with Bush’s as a remarkable feat of scientific prognostication. Mitchell’s office of the future was one in which uncompleted memos, letters, and reports would exist solely on computer, to be printed out only when a final hard copy was needed. (“ Much of the current ‘paper pushing’ in today’s offices will be replaced by people spending a large portion of their time using a computer via some personal terminal.”) He forecast the propagation of electronic mail and divined its unique ability to allow people to “communicate and manipulate information simultaneously, without the necessity of physical proximity.” The floppy disk would replace the file cabinet as the principal repository of documents and information. Dick Shoup, reporting on integrated circuit technology, anticipated the development of “smart” appliances such as toasters and alarm clocks equipped with simple but powerful chips. John Urbach’s paper on “archival memory” described digital photo-optical media resembling today’s CD-ROMs and compact audio discs. “There seems little reason to store sound in analog form,” he wrote, observing that acoustical information is easily reduced to bursts of digital bits—thus consigning the LP record to the dustbin more than fifteen years before it actually met such a fate. To be fair, many of the startling innovations posited by the Pendery authors were ringers: They were only modest extrapolations from technologies well-known throughout the research community, if not among the broad public. Mitchell’s description of tomorrow’s text-editing and office systems drew heavily from Doug Engelbart’s 1968 demonstration. Shoup’s survey of integrated circuitry scarcely ventured much beyond devices that were already on the market or known to be under active development. Still, futurists have no obligation to venture solely into the realm of magic and crystal balls; sometimes a clear vision of what lies around the next bend will do. As things stood, the Pendery Papers were important for PARC and Xerox in three ways. The first was that they implicitly embraced the immense but still widely unappreciated power of Moore’s Law. The term appeared nowhere in the Pendery Papers, but its significance permeated every page. The implication of Moore’s article had been that technologies impractical in 1965 would be commonplace within a decade or two. In the Pendery Papers PARC informed Xerox that the devices on the drawing board today would be marketable in ten years, so it was time to get ready. “This was their version of the old hunter’s saying, ‘Never aim at the ass end of a duck,’” remarked George White, Jack Goldman’s assistant, who served on Pendery’s task force, “PARC was telling us that if you want to invest in research at Palo Alto you’ve got to get way ahead. Otherwise, by the time the ripening and maturing process from your research comes through events will have overtaken you.” PARC further understood that Moore’s Law would pack its greatest wattage in the visual interaction between computer and man. Virtually every paper touched on this topic and some dwelled on it at length (Kay’s was devoted entirely to display technology). It was as though the lab had finally absorbed the lesson Bob Taylor had been pressing on it for more than a year: The computer is a communications device in which the display is the whole point. The third benefit of the Pendery Papers inured to PARC alone. “It was a matter of setting the primary focus for the lab,” recalled Peter Deutsch. “Even though in our guts nobody believed that you would be able to put a portable computer on every desk ten years from now, that was what was said by the industry trend and the curves of various things. You’d be able to put something equivalent to MAXC on everybody’s desk in ten years.”

---

At Hughes Research Laboratory in Malibu, Theodore Maiman had coiled an electronic tube around a cylinder of pink ruby polished at either end to a mirrored sheen. He touched off a flash of electrons within the coil, exciting the ruby into firing an instantaneous burst of single-wavelength red light from one end. The science of optics was never the same. Before the laser’s appearance, light was a crude implement. Optical scientists could knock it about with lenses and mirrors and sort it into its constituent wavelengths with prisms. But these processes bore all the delicacy of surgery performed with a jackhammer. By contrast, the laser cut like a scalpel. White light generated thermally—by bulbs and electric arcs—comprises all the colors of the spectrum, oscillating at different wavelengths and consisting of photons generated out of phase with one another. Under such conditions light inevitably scatters and diffuses over distance, like ocean waves spending themselves on the beach. Maiman’s ruby device, however, emitted a beam immune to the scattering effect. It had spatial coherence (all the light in the beam was the same wavelength) and temporal coherence, meaning that its photons were in phase. The laser could be “tuned,” like a radio antenna, to be so bright and fine that a beam shined from the Earth could visibly illuminate a spot on the moon.

---

For a company whose vast corporate fortune depended on the manipulation of light, Xerox remained resolutely behind the curve in exploiting Ted Maiman’s discovery. Everywhere Starkweather turned at Webster he saw projects coming to naught because they employed light sources too feeble. Whenever he pointed out that the laser packed 10,000 times the brightness of a conventional light source he encountered sneers, especially when he suggested that the new devices might play a role in xerographic imaging. Lasers were difficult to handle and burned out faster than a rick of dry timber, his colleagues responded. Bristling with electrodes and emitting bursts of blinding light, they seemed about as safe to put into an office machine as nuclear warheads. And they were expensive—$ 2,500 to $ 25,000 for a single unit. For the next few years Starkweather had no choice but to experiment on his own. His instincts told him that a beam so precise could be modulated—that is, altered in intensity—to carry information, just like radio waves or the pulses on a phone line. Suppose one could educate a light beam to reliably transmit digital bits: These could then be translated into marks on a blank sheet, a feat that would allow one to consign to paper the thoughts and images created inside a machine. Enlisting the help of a couple of lab assistants, he built a clumsy prototype, hitching a laser apparatus to an old seven-page-a-minute copier no one used anymore. Whenever he could steal an hour or two early in the morning or late at night he would run some equally clumsy tests by bombarding an unused xerographic drum with laser beams. Eventually he learned how to scan an original image and turn out a duplicate. True, his first samples were crude and pale, not at all ready for prime time. Still, they were scarcely any worse than the faded, scrawled “10-22-38 Astoria” Chester Carlson had reproduced on a coarse apparatus in his kitchen. From Carlson’s crude and pale sample, Starkweather kept reminding himself, an awesome new industry had sprung. Who was to say that his might not do the same? Nevertheless, Starkweather got scarcely more respect than Carlson had at the start of his own researches. “The theoreticians gave me every excuse,” he recalled. “All hogwash. They told me the beam would be moving so rapidly the photoreceptor would never see it. They talked about ‘photoconductor fatigue’ and asked, How will you modulate? They thought there was no practical value in it. ‘We got copiers we need to ship, you need to work on the lenses for that… Painting laser beams, these things are expensive, they never last very long and they look like a ham radio set. It’s a completely useless application. If you paint at 200 dots per inch that’s a million bits of data, where will you ever get a million bits of information?’ In 1968 that was probably a valid question. But it wasn’t a valid question if you looked at where the technology might go.” Over months and years of trying, fueled by the inner conviction that drives natural inventors, he fashioned experiments that answered every objection. He could modulate the beam by varying the power input and scan it by the clever application of a set of mirrors. He was proudest of disproving the old bugaboo about “photoconductor fatigue.” This referred to a hypothetical property of the selenium coating of the copier’s xerographic drum, the electrostatic charge of which must be neutralized by light in order for the duplicating process to work. Laboratory dogma maintained that excessively bright light would drive the neutralization effect deep into the selenium layer, like a hammer driving a nail through soft wood. Once the photoconductor thus became too “fatigued” to consistently snap back to a blank, quiescent state, one would see persistent “ghosts” of earlier copies, all transferred together to the blank paper. The objection, being strictly theoretical, was hard to discount. “It was only an inkling,” Starkweather explained, “because no one had ever tried to expose things in a few billionths of a second before.” Starkweather’s experiments proved the inkling false. He showed that bathing a photoreceptor with the laser’s extraordinarily potent beam for a fraction of a second had the same effect as applying conventional light for the much longer period employed in ordinary xerography. The brevity of the exposure canceled out the strength of the beam, and the selenium survived just fine. As for the complaints about the devices’ cost, Starkweather figured lasers were bound to come down in price. What, after all, was the laser? A neon tube with mirrors on the ends. A sign that says “Eat at Joe’s,” unfurled into a straight line. “There’s a feeling down in your stomach where you’re sure the thing has potential,” he recalled of those solitary days and nights. “You have to believe against all odds that the thing will work.” He also realized he might have an answer to a problem computer science had not yet solved satisfactorily: how to transform a stream of digital bits into something intelligible on paper. A laser could address a photosensitive drum with enough speed to print microscopic dots as fine as 500 to the inch, each one corresponding to a bit of digital data. “I said, what if instead of scanning the image in, as is done in office xerography, I actually just created the data on the computer? If I could modulate the beam to match the digital bits, I could actually print with this thing. I did some test experiments in Rochester, which my immediate management felt was probably the most lunatic project they’d ever seen in their lives. That’s when my section manager said, ‘Stop, or I’m going to take your people away.’”

---

Still, the most troublesome problem was not electronic. Instead it fell squarely within the domain of traditional optics. Starkweather knew that if the mirrored facets were even microscopically out of alignment, the scan lines would be out of place and the resultant image distorted or unintelligible, for the same reason a wobbly tape deck makes an audio-cassette warble as though recorded under water. To produce clean images, he calculated, the facets could not be out of vertical alignment by more than an arc-second—a microscopic variance. In visual terms, the mirrors could not be off by more than the diameter of a dime as viewed from a mile away. Disks fabricated to such an exacting standard would cost at least $ 10,000 each—assuming this were technically possible, which Starkweather doubted. It was true that there existed servo-mechanical and optical devices that could quite effectively redirect an errant scan back in place. But they were even more expensive and, as a further drawback, meant adding another complicated and failure-prone component to his printer. Starkweather understood that the tolerance issue was critical. If he could not solve it, he would have designed a machine that could not be cost-effectively manufactured. For more than two months he wrestled with the puzzle. “I would sit and write out a list of all the problems that were difficult. One by one they would all drop away, but the mirrors would still be left.” One day he was sitting glumly in his optical lab. The walls were painted matte black and the lights dimmed in deference to a photoreceptor drum mounted nearby, as sensitive to overexposure as a photographic plate. Starkweather doodled on a pad, revisiting the rudimentary principles of optics he had learned as a first-year student at Michigan State. What was the conventional means for refracting light? The prism, of course. He sketched out a pyramid of prisms, one on top of another, each one smaller than the one below to accommodate the sharper angle of necessary deflection. He held the page at arm’s length and realized the prisms reminded him of something out of the old textbooks: an ordinary cylindrical lens, wide in the middle and narrowed at the top and bottom. “I remember saying to myself, ‘Be careful, this may not work. It’s way too easy.’ I showed it to one of my lab assistants and he said, ‘Isn’t that a little too simple?’” It was simple. But it was also dazzlingly effective. Starkweather’s brainstorm was that a cylindrical lens interposed at the proper distance between the disk and the photoreceptor drum would catch a beam coming in too high or low and automatically deflect it back to the proper point on the drum, exactly as an eyeglass lens refocuses the image of a landscape onto a person’s misaligned retina. “I ran to the phone and called Edmund Scientific, my supply house, gave them my credit card, and bought ten bucks’ worth of war surplus lenses,” he recalled. “I could hardly sleep the two days before they arrived. But then they came, I put them in, and sure enough they worked.” The lens scheme was foolproof. It involved a simple physical relationship, so it could never fail. It had no moving parts, so it could never malfunction. And it permitted the polygonal disks to be stamped out like doughnuts—not at $ 10,000 apiece, but $ 100.

---

That half was solved by the invention of the so-called Research Character Generator (RCG), another healthy piece of iron and silicon, by Lampson and a newly hired engineer named Ron Rider. The RCG, which stood several feet high and nineteen inches wide, and housed 33 wire-wrapped memory cards holding nearly 3,000 integrated circuits, was a sort of super memory buffer, spacious enough to accept a digital file from a computer, evaluate it scan line by scan line, and tell the printer which dots to print at which point. This generated on paper an image created by pure electronics. Today this procedure is trivial. Memory is so cheap that the computer and printer both come with enough to hold several pages at a time. As a page comes in from a word-processor program, it is fitted into a print buffer the way craftsmen of the old printing trades clamped lines and columns of leaded type into rectangular frames. Once in memory, the page image can be manipulated in an almost infinite number of ways. It can be fed to the printer narrow or wide end first, backwards, upside-down, or wrapped around a geometrical design. The most unassuming desktop computer can store character sets in dozens of font styles and sizes, any of which can be summoned at will and applied to a document as a paintbrush swipes color at a wall. Nothing like this was simple in 1972 because of the cost of memory. Nor was it enough for Rider’s machine to generate only the bland standardized ASCII text of conventional line printers. The RCG had to incorporate a large number of custom typefaces that were to be drawn by hand, converted into digital bits, and stored somewhere in memory until needed, as if on an electronic shelf. This meant an exponential increase in the complexity of the task. ASCII characters were all the same size and each fit into the same squared-off shape. The only formatting a conventional document normally required was a command instructing the printer when to move to the next line. By contrast, the custom-designed characters PARC desired to print would be proportionately spaced: some fat, some thin, some reaching above the print line, some dangling below; some roman, some italic, some BOLD. Finally, the character generator had to adapt to the Model 7000’ s system of feeding in pages wide-edge-first, which moved paper through the machine at a faster rate. For copiers this posed no problem—one simply aligned the originals along the same axis. For a printer, however, it was a horror. The image coming from the computer would somehow have to be rotated before it could be printed out. Instead of printing a page in prim linear order like a typewriter, SLOT would have to reproduce the characters in vertical slices, somehow keeping its place on twenty or thirty lines of print per page. Rider ultimately came to see the proliferation of complications as a blessing in disguise. “It forced you to think about the problem of printing in a much more generalized fashion, so the solution turned out to be much more robust.” Despite its name, the research character generator was less about delivering images character-by-character than about transmitting digitized images in whatever form the computer dictated. Like so much PARC developed in those first few years, this turned out to be the answer to a multitude of questions no one was yet even asking.

---

Starkweather’s SLOT and Rider’s character generator were two of the four legs of the complete interactive office environment PARC was creating on the fly. In the same period Thacker, McCreight, and Lampson were building the Alto; Alan Kay and his Learning Research Group were designing a graphical user interface aimed at making computers intuitively simple to use; and Bob Metcalfe and David Boggs were designing a network—the Ethernet—to tie all the other components together. “We had in mind that you ought to be able to create a file on the Alto and ship it via the Ethernet to a print server [that is, a communal computer managing everyone’s print orders], which would convert it to a raster and print it out,” Rider recalled. When it was finally implemented, the whole array would be known by the rather inelegant acronym EARS, which stood for “Ethernet-Alto-RCG-SLOT.”

---

Taylor’s predecessors had bequeathed him the axiom that the best way to manage research was to select the best people in a given field and set them loose. Scientists with the lofty skills ARPA demanded, Ivan Sutherland said, “are people who have ideas you can either back or not, but they are quite difficult to influence. You can maybe convince them that something’s of interest and importance, but you cannot tell them what to do.” On the other hand, you can find a way for them to tell each other. The uncompromising give-and-take of Taylor’s ARPA contractor meetings lent itself to reproduction at PARC in the form of “Dealer.” The name derived from the book Beat the Dealer, by Edward O. Thorp, an MIT math professor who had developed a surefire system for winning at blackjack—“ beating the dealer”—by counting the high-and low-value cards dealt out in hands. (This truly effective system would make the unassuming Ed Thorp the godfather of professional blackjack card-counting.) Taylor was not much of a blackjack buff. What interested him about Beat the Dealer was its compelling metaphor of a doughty individual fielding the challenge of a group of trained and determined adversaries. In casino blackjack the dealer plays against everyone at the table. In Taylor’s variant a single researcher would propose an idea or project, then stand alone to defend it against dissection by his peers. Dealer was soon institutionalized as the beating heart of CSL’s professional organism, a time when the entire lab would gather in a room furnished with the beanbag chairs that Peter Deutsch and his wife, Barbara, had discovered at a friend’s shop in Berkeley. The meetings, which were usually on a Tuesday (although the designated day changed from time to time), were scheduled more or less at lunchtime and generally lasted an hour. Attendance was mandatory for all of Taylor’s subordinates, the only lab rule he rigidly enforced,

---

It was the dealer’s prerogative to set not only the topic of discussion, but the rules of debate. “I wanted to have conditions where someone could get up to the table and set rules as czar,” Taylor recalled. “You could say, no interruptions; or interrupt whenever you want. Or I’ll only debate x, y, or z; or only righthanders can argue.” The discussion topics were similarly unconstrained. Certainly they tended toward issues of importance to the lab, but that category was broadly defined. Bob Flegal, a CSL graphics expert, once demonstrated for his colleagues how to take a bicycle apart and lubricate the parts,

---

“If someone tried to push their personality rather than their argument, they’d find that it wouldn’t work.” But the argument had best be carefully thought out. Anyone trying to slip an unsound concept past this group was sure to be stopped short by an explosive “Bullshit!” from Thacker or “Nonsense!” from the beetle-browed ARPANET veteran Severe Ornstein. Then would follow a cascade of angry denunciations: “You don’t know what you’re talking about!” “That’ll never work!” “That’s the stupidest idea I’ve ever heard!” Lampson might add a warp-speed chapter-and-verse deconstruction of the speaker’s sorry reasoning. If the chastened dealer was lucky (and still standing), the discussion might finally turn to how he might improve on his poor first effort.

---

Bob Metcalfe, who arrived at CSL in 1972 with the reassuring credentials of a Harvard and MIT education. Metcalfe was acerbic and free-speaking, a man who never met an ego he couldn’t pierce. At Dealer his radar often detected the unmistakable “ping” of people pulling rank. “I’m being cynical now, but if you were from Berkeley or MIT or, especially, CMU, you’d give your talk, you’d get some questions, you’d get congratulated, and you’d get a job offer,” he said. “But if you were some poor schmuck from the University of Arizona, they’d grill you and it was all over. In other words, if the department head at CMU said you were cool, that was good enough for them.”

---

Butler Lampson. His combination of a razor-sharp intellect with peerless debating skills raised the bar for new ideas to an intimidating height. It was not impossible to win an argument with Lampson, but it was not at all rare for him to win one even when he was wrong. Even as practiced a navigator of Ideaspace as Alan Kay could be backed to the wall when one of his flights of fancy came up against Lampson’s rigorous command of pragmatic engineering. Routed in the battles, Kay sometimes had to retreat and regroup for another run at the fence. “I can’t ever remember winning an argument with Butler on the same day,” he said later. “I could win quite a few on the second day. His mind worked about twice as fast as anyone else’s.”

---

For the extended family of the Computer Science Lab, Bob Taylor served as a sort of social director. On weekends there might be touch football (quarterback: Bob Taylor) or marathon sessions of “Diplomacy,” a board game whose framework of negotiation, alliance, and betrayal fed the host’s appetite for intrigue, at his Palo Alto house. “That was great fun, when you had nothing to do for a whole eight or ten hours on a Saturday or Sunday,” one participant recalled. This was the sunny side of Taylor’s personality. When he was playing the role of paterfamilias, as opposed to sneering at the physicists or disputing a football ref’s call with an opponent’s shirt grasped in his fist, one could appreciate the 95 percent of the time he could be “an absolutely charming person,” as Jones recalled, without thinking of the other 5 percent when he was a rude and arrogant beast. Even his beleaguered superiors could laugh at his foibles and persnickety habits, as they did one Halloween when half of CSL came dressed as Bob Taylor, in nearly identical plaid slacks, blue blazers, and white turtleneck sweaters, then sat together at a table in the cafeteria with pipes in one hand and Dr Peppers in the other. “There isn’t an organization newly begun where you don’t find those honeymoon years where there’s a special bond among people,” reflected Jeffers, who recognized the phenomenon from the Peace Corps. “It was true there, it was true in PARC. It’s true in anything that’s new. It’s a great period. Everyone should be a part of something at the beginning.”

---

They called the process of informal collaboration by the name “Tom Sawyering.” Like Tom with his paintbrush and whitewash, someone would set forth his idea or project—whether it was in a formal meeting or a hallway bull session was unimportant—to mobilize a few intrigued colleagues in an attempt to make it happen. If you saw a glimmer of how to implement a new operation in microcode, you would gather a few expert coders in a room and have at the problem until every whiteboard in the place was filled with boxes and arrows and symbols as arcane as Nordic runes. If you had a big project with a lot of soldering to be done, everyone who knew how to wield a soldering gun strapped on his holster. If an idea worked, the team stuck together for the next three or six months to complete the job; if not, everyone simply dispersed like free electrons in search of a new creative valence. Thacker viewed this system as “a continuous form of peer review. Projects that were exciting and challenging received something much more important than financial and administrative support. They received help and participation… As a result, quality work flourished, less interesting work tended to wither.” In this spirit Systems Science Lab engineers wrote code for Computer Science Lab hardware, CSL designers helped SSL build prototypes, and the General Science Lab’s physicists chipped in with valuable insights into material properties and electrical behavior (as when Dave Biegelsen told Starkweather how to use sound waves to modulate a light beam and got his offhand suggestion incorporated into the world’s first laser printer).

---

At one point Tom Sawyering even begot an audacious extracurricular project. This was the so-called “Bose Conspiracy,” which was hatched at a poker game at Rick Jones’s house. Jones, Kay, Thacker, Dick Shoup, Chuck Geschke, and a couple of others had fallen into a discussion of the merits of stereo speakers. Kay was a particular fan of the state-of-the-art Bose 901s, which came with their own electronic equalizer and cost $ 1,100 the set (in the pre-oil shock dollars of the early 1970s). He was also the only one in the group who owned a pair, having acquired them on his PARC budget as part of a real-time music synthesizer his group was developing. “You know,” someone said as cards riffled in the background, “there’s no reason why we couldn’t make the electronics work just as well. And for a lot less money, too.” Appropriating a basement room in Building 34, the group took apart Kay’s speakers and painstakingly analyzed the design. They bought cone speakers from the same Kentucky factory that supplied them to Bose, and on a shrieking diamond-toothed radial saw in Jones’s garage they cut and shaped the sound baffles out of high-density particle board. (The marathon session left Kay covered with an inch-thick coating of sawdust and Jones with a lifelong case of tinnitus.) Then they apportioned the assembly tasks—one conspirator handled the soldering, another installed the speaker cones, and so on—the same way they had distributed the tasks on MAXC, which happened to be running contentedly in its own air-conditioned room a few doors away. All told, they manufactured more than forty pairs at $ 125 each. The buyers among their PARC colleagues could customize the units with their choice of grille cloth but were otherwise challenged to tell the knockoffs apart from the real thing. No one could. “It was so typical of PARC,” Kay recalled. “If you didn’t know how something was done, you just rolled your own.”

---

“The lights would all be lit and dozens of people around, even it if was nine or ten at night,” he recalled. “Often they were playing computer games. Now, just remember, in those days computer games were not what they are today. This was a new thing. These guys were literally inventing computer games and learning how to use the machine.”

---

Alan Kay, who Brand introduced as something of a hacker eminence offering his own definition of “the standard Computer Bum”: “He’s someone about as straight as you’d expect hot-rodders to look. It’s that kind of fanaticism. A true hacker is not a group person. He’s a person who loves to stay up all night, he and the machine in a love-hate relationship… They’re kids who tended to be brilliant but not very interested in conventional goals.” Kay’s assessment of the computer scientist’s professional mores could not have been better designed to raise hackles in the Stamford executive suite. “People are willing to pay you if you’re any good at all,” he observed, “and you have plenty of time for screwing around.”

---

Kay had the feeling he might finally be within striking distance of turning some of his great ideas into reality. He had reworked his Dynabook concept into something he called “miniCom,” a keyboard, screen, and processor bundled into a portable, suitcase-sized package. Meanwhile, the software aces he had brought together as PARC’s Learning Research Group had turned his outline for a simplified programming language into real code, to which he gave the characteristically puckish name “Smalltalk.” (Most programming systems “were named Zeus, Odin, and Thor and hardly did anything,” he explained. “I figured that ‘Smalltalk’ was so innocuous a label that if it ever did anything nice people would be pleasantly surprised.”) Kay’s team had already demonstrated Smalltalk’s implicit power by running rudimentary but dazzling programs of computer-generated graphics and animation on a video display system built by Bill English’s design group. Kay himself was a compulsive promoter, producing a steady stream of articles and conference abstracts, often illustrated with his own hand drawings of children in bucolic settings playing with their Dynabook, to proclaim the death of the mainframe and the advent of the “personal computer.” By the spring of 1972 he was ready for the next step. Having drawn on Seymour Papert’s LOGO for some of Smalltalk’s basic ideas (although the two languages worked much differently under the surface), Kay was anxious to give it a Papert-style test run. That meant giving children, its idealized subjects, a shot at performing simple programming tasks on miniComs. He figured he would need about thirty of the small machines, to be built by the Computer Science Lab’s crack hardware engineers. The only thing left to do was persuade CSL to take the job. That May at a CSL lab meeting, Kay made his pitch. As the lab staff lounged in front of him in their beanbag chairs, he laid out the argument for building the world’s first personal computer.

---

“Jerry Elkind knows enough to be dangerous.” At this moment he pronounced the words that most CSL engineers had learned to dread as his kiss of death. “Let me play devil’s advocate,” he said. He proceeded to pick apart Kay’s proposal in pitiless detail. The technology was speculative and untested, he pointed out. To the extent that the miniCom was geared toward child’s play, it fell outside PARC’s mandate to create the office system of the future. To the extent that it fell within that mandate, it was on entirely the wrong vector. Perhaps Kay had not noticed, but PARC had not yet finished exhausting the possibilities of time-sharing. That was the whole point of building MAXC, which was after all a time-sharing minicomputer. As Kay recalled later, the sting still fresh: “He essentially said that we had used too many Green Stamps getting Xerox to fund the time-shared MAXC, and this use of resources for personal machines would confuse them.” And what about the issue of PARC’s overall deployment of resources, Elkind asked. A major office computer program was already well under way in Kay’s own lab. Had Kay given any thought to how his project might fit in with that one? Elkind was referring to POLOS, the so-called “PARC On-line Office System,” which was Bill English’s attempt to reproduce the Engelbart system on a large network of commercial minicomputers known as Nova 800s. He was correct in stating that POLOS ranked as PARC’s official entry in the architecture-of-information race. This was so in part because English had cannily put a stake in the ground with a round of purchase orders for the Novas, which committed Xerox to following through. The small, versatile machines were already proliferating at SSL like refrigerator-sized Star Wars droids. But Kay considered POLOS irrelevant to his project. POLOS was explicitly a big-system prototype, an expensive luxury model as far removed from the homey, individualistic package Kay had in mind as a Lincoln Town Car is from a two-seat runabout.

---

Now English volunteered some further advice to his wounded young colleague. Among the PARC brass, Kay lacked credibility. All the way up to George Pake he was regarded tolerantly as a sort of precocious child, engaging enough in his place but profoundly in need of adult supervision. His reputation as a dreamer only made it easier for bureaucratic types like Jerry Elkind to dismiss his ideas without affording them serious scrutiny. English informed Kay, in essence, that his barefooted treks through Ideaspace would no longer do. He had to learn to develop written research plans, compile budgets, and keep notes—in short, to look and act like a serious researcher. Kay took the advice to heart. Over the next few months he drafted a detailed plan for a music, drawing, and animation system to teach kids creative programming on Novas. He did not abandon his cherished miniCom, but recognized that he would have to reach the grail via a series of smaller steps and commit himself to a program of several years. The effort bore fruit. By summer’s end he had acquired a $ 230,000 appropriation to equip a bank of Novas with character generators that produced text and simple graphics for display on a high-quality screen. His small group of learning software specialists had been about to begin developing the programming environment for this jury-rigged system when Lampson and Thacker knocked at his door with a different idea.

---

the architecture of his cherished Dynabook, or miniCom, or Kiddicomp—whatever he was calling the thing in its latest incarnation—corresponded neatly with their own visions of the ideal personal computer—for Lampson a suitcase-sized MAXC with a component cost of about $ 500; and for Thacker a computer with the Nova 800’ s capabilities and ten times its speed. The notions of all three intersected at one common goal: a fast, compact machine with a high-resolution display. “The thing had to fit in a reasonable sized box and it couldn’t cost too much,” said Lampson. “Small and simple was critical, because the whole point of it was to have one for everybody.” By combining the latest electronic components coming into the market with their own powerful intellects, they might just pull it off. Not the Dynabook in all its interactive glory, perhaps, but a giant leap in the right direction—in Kay’s words, an “interim Dynabook.”

---

The Alto’s screen, whose dimensions and alignment replicated that of an 8 ½-by-11-inch sheet of paper, produced such a vivid impression that the lab’s modest construction plan was soon expanded. In the end Xerox would build not thirty Altos, but nearly two thousand. The Alto was by no means the fastest or most powerful computer of its time. MAXC could blow it away on any performance measure in existence and for a considerable time remained the machine of choice at PARC for heavy-duty computation. Even without the burden of illuminating the full-screen display, the Alto ran relatively slowly, with a processor rate of less than 6 megahertz (the ordinary desktop personal computer as of this writing runs at a rate of 400 MHz or faster); the display slowed it further by a factor of three. But the Alto’s great popularity derived from other characteristics. To computer scientists who had spent too much of their lives working between midnight and dawn to avoid the sluggishness of mainframes burdened by prime-time crowds, the Alto’s principal virtue was not its speed but its predictability. No one said it better than the CSL engineer Jim Morris: “The great thing about the Alto is that it doesn’t run faster at night.” Then there was the marvelous sleekness of its engineering. To some extent this was an artifact of Thacker’s haste, for his tight deadline erased any impulse he might have felt to create a second system variation on MAXC. There was simply no opportunity for biggerism. Instead, to save time and money Thacker and his team went entirely the other way. Alto was like a fine timepiece somehow assembled from pieces of stray hardware lying around the lab. Rather than design new memory components, they ingeniously reused boards that had already been built for MAXC. Ed McCreight revisited his own design of the MAXC disk controller and managed to strip out a few more circuits for the Alto. Even the display monitors were appropriated from POLOS, which had fallen so far behind schedule that its specially ordered video display terminals were still sitting around in boxes.

---

Class One disagreement. “That’s when two people disagree and neither can explain to the other person’s satisfaction that other person’s point of view,” he said. “A Class Two disagreement is when each can explain to the other’s satisfaction the other’s point of view. Class Two disagreements enable people to work together even when they disagree. Class One is destructive. Most disturbances and international crises and most of the pain and suffering and difficulty in the world are based on Class One disagreements.

---

This involved finding a simple and reliable way to connect PARC’s Altos to each other. The local network was the sine qua non of interactive distributed computing, Taylor believed: He was after more than the symbiosis of one man and one machine, but rather the unique energy sure to issue from joining together a multitude of people and machines all as one. Unfortunately, none of the network architectures then in use suited PARC’s specifications. The ARPANET was too large-scale and required too much extra hardware to link computers together in discrete local networks at a reasonable cost. IBM and several other computer manufacturers had developed their own proprietary systems, but they were specifically tailored to their own machines and difficult to adapt to others. They also tended to break down when the local loop got too large. The POLOS group’s adaptation of a network technology provided by Data General for its Nova minicomputers underscored these shortcomings. The network’s maximum capacity was fifteen computers. POLOS’s attempt to double the number had produced a multi-tentacled horror of cable and hardware. “We were able to network up to 29 Novas, but that was the limit,” Metcalfe recalled. “The ultimate 29-Nova daisy chain had twenty-eight 40-conductor cables, sixty 40-conductor connectors, and the nasty habit of crashing if any one of these fragile devices was disturbed.” The basement room at PARC where all these cables came together was aptly labeled the “rat’s nest.” Obviously this would not do for Taylor, who envisioned a system linking hundreds of Altos. His other specifications were similarly stringent. The network had to be cheap—no more than 5 percent of the cost of the computers it was connecting. It had to be simple, without any fussy new hardware, in order to promote long-term reliability. It had to be easily expandable—unlike POLOS, where adding a Nova meant taking down the network and splicing a new line into the rat’s nest. And it had to be fast, because it would be feeding files to Gary Starkweather’s swift laser printer and would need to keep up.
