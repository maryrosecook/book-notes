# Thinking, Fast and Slow, Daniel Kahneman

(Rough notes.)

* The mind is better able to understand something as a description of what it does than a description of what it is ie what properties it has. Eg system 2 vs mental arithmetic
System 2 can program system 1 with things to look out for - white haired people, words that begin with a certain letter

* The optical illusion with the lines with chevrons on them. System 2 Knowing they are actually the same length allows the mind to make the correct judgement but the system 1 intuition persists and must be effort fully overridden. It is impractical to have system 2 being vigilant and overriding all the time so system 1 needs to take most of the burden

* Exercising self control or overriding system 1 saps energy and makes it less likely you will expend effort in successive tests. Separate from this, separate from this there is only a certain hardness with which you can work before you reach a limit eg the add three task so that is why people break tasks into bite size chunks. But the separation between max hardness and max willpower is there but I am not fully clear on their relationship and differences yet.

* Resisting intuition is a good thing because it is wrong a reasonable proportion of the time.
You can prime someone for ideas by putting a related word into their mind but this also primes them (less strongly) for ideas related to the secondary ideas

* If you hear an assertion and it is familiar but you can't remember where you heard it and you have no grounding in the subject, you can only go on the feeling of cognitive ease to judge its truth
Don't use complicated vocabulary or constructions needlessly - it is less likely to be believed

* Strongly linked by logic, associated with other beliefs or preferences, or comes from source you trust or like will feel cognitive ease. These all ckme from system 1 apparently. Note all of these are valid reasons for believing something. But the other factors like ease of reading, familiarity, rhyme will bias you towards statements too. It is difficult to tease apart the source of cognitive ease, but not impossible.

* Cognitive strain is experienced when system 2 is operating. But also system 2 is more likely to be made to engage when cognitive strain is experienced whatever the source. This brings about the shift from casual and intuitive to engaged and analytic.

* System 1 can't rhink statistically, only causally. This means it uses causation as a justification for things that should be thought of statistically.

* When no context exists then system 1 supplies one. Eg ann approached the bank.

* Also your mind chose a financial institution rather than a river bank because You probably spend more time going to banks than floating on rivers. Recent events and current context guide this.

* Also note that system 1 did not keep track of the alternative contexts. You are never aware of them. It makes a definite choice.

* System 1 is biased towards believing things but system 2 is in charge of doubting - of turning a belief into an unbelief.

* System 2 tries to find data that confirm a hypothesis rather than doing what philosophers of science recommend which is to look for data that refute the hypothesis.

* Halo effect. Start making positive assumptions about people you already like. Eg Joan who is personable in conversation when you are asked if she is generous you are predisposed to think she is.

* List of adjectives about Alan and Ben. Same for each but for Alan the good ones come first. This inclines you to like him more than Ben. Even further, the ambiguous adjectives like stubborn get a positive spin for Alan because their context is a good person. The ambiguity is again system 1ed so you are not even aware of the ambiguity unless you think about it.

* Wisdom of crowds only works if the people make their judgements independent of each other. Otherwise they influence each other. Meetings are influenced by people to speak early and convincingly.

* In a dispute, The presentation of only one side, even when people know they are only hearing one side, and even when the evidence not presented could have been inferred, led to people believing the presenting side and being more confident in their belief than people who heard both sides.

* System 1 only cares about the coherence of the story, not the amount of information or (I think) the believability of the information.

* System 1 thinks in terms of prototypes and sets. This means it can tell you what the average of a group is eg average length of lines well enough to change another line to similar length. But it can't do sums of values because it doesn't know very well how many prototypes there are.

* People were asked how much they would give to save 2000, 20000 or 200000. People were only asked about one amount. The dollar amounts were all around 80. This shows system 1 can't deal with summing. It just used a prototype to picture a sad dying bird to assess. It didn't picture lots of them.

* Can also have scales of intensity eg from darkest to lightest longest to shortest worst to best. Values can be matched from scale to scale eg when a certain loudness of sound is associated with the idea of theft, hearing a much louder sound to represent the punishment makes people feel uncomfortable.

* Some of these basic assessments like comparative length are involuntary. Other non basic assessments eg how happy am I have to be undertaken voluntarily.

* Vote and note vs vote and goat. Subjects asked to press button as quick as pos if rhyming. Slower for second because the incongruous spelling was automatically notes by system 1 and this slowed it down. Sometimes system 1 answers questions that it wasn't asked and sometimes this slows it down.

* Mental shotgun (pulling up many partly related things eg like spelling as well as sound) and intensity matching are the reason we can have strong intuitions about things we know little about. Eg the guy who evaluated a company's chances of financial success as favourable because he liked the design of their cars.

* Rare to have no answer to a question. The mind produces one with what it has. An exception is something like 17 times 24

* If a satisfactory answer to a hard question is not found, system 1 will find a related question that is easier and answer it instead.

* Target question: how happy are you with your life. Heuristic question: what is my mood right now. The heuristic queTion comes from the mental shotgun. Intensity marching lets the heuristic question's answer be quickly translated to the target question.

* It's harder to maintain doubt than slide into certainly (more on why to follow)
Hot hand in basketball doesn't exist. Players who score several baskets in a row are not more likely to score more in a row. It's just random. There is variation in the overall scoring rate of different players, but the distribution of the baskets a player will make doesn't go in streaks.

* How long do you wait before deciding there is a pattern in events? Likely that you should wait longer than it takes your intuition to see events as a pattern

* Also note that easiness of constructing a causal story is a factor in seeing patterns. Eg small schools found in study to be better, and it's easy to think up reasons why this might be.

* We pay more attention to the message of an incident than the reliability of the source

* Law of small numbers - small sample size hides randomness eg cancer incidents high and low on small communities.

* Anchoring. Wheel of fortune numbered to 100 that only stops on 10 or 65. It was spun, students wrote down num then asked to estimate percentage of African nations in the un. The ones that saw ten estimated much lower.

* Anchoring is both system 1 and 2. System 1 hears a number like is gandi's age lower than 144 and tries to produce a world where that is true by summoning associations eg an image of gandi as old. System 2 adjusts down or up from the anchor. Down adjustments eg after getting off motorway produce answers that are higher.  Up adjustments produce ones that are lower.

* Anchors that are obviously random still produce an anchoring effect.

* If someone tries to anchor you by proposing a high or low number, you can't just come back with an equally out of range suggestion in the opposite direction because this will create an unbridgeable gap. Instead, storm out of threaten to and say that you won't continue negotiations with that number on the table.

* Another way to resist anchoring is to engage system 2 by consciously thinking of examples that are reasons to reject the anchoring number - eg only two bedrooms.

* If negotiation is only about one axis eg price, the first person to suggest a number has an advantage because they can anchor.

* Another example. Public companies have lobbied to limit the damages claims to a certain number. This seems to benefit them but could also benefit claimants if the amount they would have sought is much lower than the limit. But ultimately his benefits large companies where the damage claims are higher more than small companies.

* A plan anchors you on best case scenarios. Tito combat this, think about ways the plan might go wrong.

* The availability bias affects both system 1 and system 2. We have an intuition about a category's size even without thinking of examples. Eg how many words can be created from these two sets of letters. And being able to explicitly think of examples with system 2 also biases you to think a category larger

* Fluency of retrieval of instances is more important than number retrieved the less the recaller is involved. Eg the students who were asked to recall instances of things they do to support their cardiac health.

* There are two opposing forces affecting the availability heuristic. The more instances you retrieve the more confident you feel that the set is large but also the harder it gets to retrieve instances and so the less confident you get. The more involved you are on the category, the more number becomes important.

* This is because the more you are involved the more effort you put in and so the more system 2 can check the content of your memories.

* The reason fluency of retrieval affects things is because your system 1 has a higher expectation of ease than will usually be the case and so it is surprised and so it thinks the truth seeking it is trying to find must not be there. If you are given an explanation of why you can't think of instances eg this music will impede your recall then you put less emphasis on the fluency of recall.

* Affect heuristic. Something you like is seen as less risky AND more beneficial than something you like less. When people were given a list of benefits of a technology, their assessment of its benefits went up but also their assessment of its risks went down. When given list of risks, perception of benefits went down as well as risks going up. Even when person just told that risks are mild, their perception of benefits goes up. This is emotions taking over from rationality.

* Base rate of number of students per subject completely ignored when given to people asked to rank likelihood a student is studying different subjects when they are also given a description of the student's personality that fits stereotypes. System 1 produces the association between the student and the stereotypes of the groups and system 2 doesn't work hard enough to override this and think about the base rates. Subbed easier question for harder one.

* System 1 and system 2 both at fault. System 1 for producing the bad intuition and system 2 for endorsing it. System 2 might be being lazy or might be ignorant of the importance of base rates and/or might be neglecting to weight the people description lower because of the advice that the personality test is unreliable.

* System 1 either immediately rejects information as a lie or it thinks it is true.

* The unreliable personality information should very slightly affect your estimates because there is a chance it's true, but the base rates should be the main info you use to estimate.

* Described Linda - feminist, political, vegetarian. What uni did she go to? Everyone says Berkeley but Berkeley student body small compared to all students in the us.

* Is she a bank teller? People say very unlikely. Is she a feminist bank teller? People say maybe. This makes no sense because a more specific description cuts down the people who might fit (assuming there are feminist bank tellers). Asked to rank likelihood of Linda being in a category one set of people have the same ranks when asked to do by likelihood as another group asked to do by likeness. Also these rankings put feminist bank teller higher than bank teller. This true even when feminist bank teller and bank teller included in a single art of rankings. Conjunction fallacy. This may come from a conflation of probability with plausibility. Latter is a good story that meets the requirements of associative coherence. Linda the feminist bank teller is plausible.
Coherence comes from detail and richness, not just characters.

* Conjunction fallacy where system 1 uses several characteristics as or groups rather than and groups to produce a higher probability. It happens when the several categories are more representative of a thing. Eg likelihood born Borg (best tennis pkayer of day) will lose first set rated lower than likelihood he will lose first set and win match.

* Can neutralise conjunction fallacy by thinking of numbers of people (eg how many of 100 people have had heart attacks and how many of 100 have had heart atracks and are over 55 vs what percentage of...) this works because system 2 can think spatially and filter a room of people.

* Conjunction fallacy happens probably because system 2 is lazy and doesn't fully engage with the problem. If it is made to fully engage, people usually come up with the right answer. But also the problem is that representativeness can block the application of using the logic rule

* Distinction between statistical base rates and causal base rates. Former is about frequency in whole population eg percentage of cabs that are blue. Latter is about percentage for an individual eg the witness is correct about cab color 80% of the time. In humans, causal base rates trump statistical base rates.

* Stereotyping (causal base rating), as long as the base rates are correct, is useful info. Society decides to reject this heuristic for judging individuals in order to make a fairer society at the cost of ignoring some potentially useful information.

* Regression to the mean. It is better for the person being trained for the trainer to praise good performance than publish bad performance. But the world teaches the opposite. If someone does well in comparison to a group they are in, they are more likely to do badly on the next go because they probably had luck in their favor. If someone does badly in comparison to a group they are in, they are more likely to do well. This means that punishing bad performance seems like a better idea because the bad performer is likely to improve on the next go round.

* Correlation and regression to the mean are the same concept from different perspectives. Whenever there is imperfect correlation, there will be regression to the mean.
Highly intelligent women tend to marry less intelligent men. The mind immediately searches for a causal explanation. But it's just regression to the mean.

* regression to the mean being about luck in the golfer example seems to actually be about correlation between performance on day 1 vs day 2.  Skill plays a part, but there is not a perfection correlation between skill and performance.

* example of julie.  she read fluently at age 4.  what will her gpa be after her first year of college? intuitive answer intensity matches from perception of how clever a 4 year old she is to the gpa - 3.7, 3.8 or so.  better idea is to use correlation because intuition assumes perfect correlation.  take average gpa for all students. think of all factors that affect reading age and all factors that affect gpa.  think of overlap between them.  author came up with 0.3 (correlation).  move 0.3 up from the mean to get estimate.

* The only problem with this approach and the "stick closer to the base rate" approach  is that you will never predict an extreme outcome eg you will never get a long shot right like correctly predicting s promising startup gets big

* In Some cases it is much worse to be wrong in a particular direction eg missing the next Google as a venture capitalist. In these cases a number of incorrect overly confident  predictions are ok because the losses are small. Another way to solve this problem is to out a range on your predictions eg expected returns on an investment

* Eg hiring. Kim interviewed brilliantly but has no track record. Jane interviewed less well than Kim but has a track record. Intuition goes for Kim because she left a better impression. But law of small numbers (more info for Jane so extreme result less likely) means should do regression to the mean means you should choose Jane.

* The black swan is about creating narratives based on scant evidence and then using them to predict the future. It's also about how success may well be because a series of decisions all turned out well, rather than superior decision making ability eg Google's rise. Our minds are bad at seeing things that didn't happen eg all the search companies that failed because we want easy to understand narratives.

* Difference between skilled rafter avoiding rocks and Google succeeding? Rafter has gone down 100s of times whereas google founders only built a company once.

* Hindsight bias. We ascribe a positive or negative spin on characteristics depending on how things are going. Eg the CEO who is described as firm when the company is doing well and rigid when it's not.

* Team of soldier strangers had to maneuver log over wall. They am were assessed for leadership. Who takes control but is ignored. Who takes control later and does well. Who follows. They discovered that their predictions of who would excel at officer training school were only a little better than random. But the next day, they felt just as confident and made just as unequivocal judgements. This examples incorporates the illusion of validity, and substitution fallacy and the representative bias (only one hour observed) and non regressiveness

* Confidence is a feeling based on coherence and cognitive ease. A statistical piece of information like the above is not a feeling and so it's hard to incorporate.

* Amateur investors do worse than random because the sell stocks that have recently appreciated (which are likely to go up more in the short term) and buy recent losers (which are likely to go down in the short term)

* In authors' study of pro investors, they did the same as random in relation to the market changes. But when author told firm this, they kind of swept it under the rug.

* People take on board the lesson that two lines - one with fins pointing in and one with fins pointing out - are the same length even through they  don't look it. They still see the illusion but they change their belief they are different lengths  and behavior by reporting they are the same. But the stock traders don't take on board an analogous lesson. This is because they use high level skills to analyze the market and so it feels like they are exercising skill. What they don't know or accept is that their work is being put into answering the wrong question - what are the business prospects of this firm. The right question is: is this information already incorporated in the price of the stock? The confidence in their predictive abilities is a feeling not a judgement because it relies of cognitive ease and associative coherence. The belief of the traders is also supported by their community of fellow believers.

* We construct a narrative of history which makes us think we understand it. But we exclude all the things that didn't happen and so we are hugely overconfident in our understanding.

* Don't expect much from pundits making long term predictions. 29 year study done with pundits (experts) journalists (slight experts) and lay people making long term predictions found that journos did very slightly better than random and experts did worse because they were 4overconfident. There are too many factors in long term planning to be accurate. Only short term planning is somewhat reliable.
experts given 45 minute assessment period of new students forecasted their grades a year into the future worse than a rule-based method based on automatic testing (SAT, one aptitude test) (the teachers also had access to the automatic test results).  This has been studied hundreds of times in many fields from recidivism to psychiatric evaluation.  In 60% of cases, the experts do worse than the rule-based assessment of automated test results, in the rest, the experts do the same. These studies only studied fields where PREDICTABILITY IS POOR

* Wine example of above.  Man analyzed new wines by how hot and dry the summer was in which their grapes were grown.  Found it had a 0.9 correlation to the price many years in the future.  WAY outperformed the wine experts who took all sorts of other stuff into account.

* This is because people work up complex theories that are too complicated to be reasoned through. Also because people are inconsistent in their evaluations - when asked to evaluate same info twice they frequently give different answers eg radiologists reviewing X-rays give different answers 20% of the time when asked to evaluate on separate occasions - this is probably because of the effect of context on system 1. And because they give too much weight to their impressions that are context dependent and they are overconfident in in comparison to the simple measures.

* Multi value analysis that uses stats to assign weights to predictors abd combines the weights into a formula is hardly more accurate than assigning equal weight to all predictors. This is because in the latter case the simpler formula is not affected by accidents of sampling.

* Experts overestimate their skill because they judge themselves on short term predictions during sessions eg how a patient will respond to a question during a therapy session and they are often right. But the long term tests are long term which the experts have less practice at predicting.

* Interviewed army recruits and used specific factual questions to score them on specific factual traits - masculine pride, responsibility etc. only asked them about the past and not now. Their scores were combined with a formula. Predictive ability for their effectiveness as army recruits a few months in the future was much fm higher than the previous method of generalized interviews where future ability was predicted by the interviewer. However, in the new method the interviewers were asked to also evaluate the candidate. Their predictions were just as good as the formula.  The lesson here is that intuitions based on standardized information collection are a good source of intuition.
Expertise is just recognition of circumstances.

* Experts come up with a solution when system 1 pattern matches to experience. They use system 1 to simulate the solution and accept it if it's good enough. If it's not system 2 comes up with a new solution and that is simulated and so on.

* The expert rarely knows how the solutions come to them

* This learning comes from experience. It is possible to learn very quickly. Eg humans are very fast to learn fears. Learned fear can come from a single bad incident eg food poisoning putting you off a dish forever. Learned hope is Lesrnt quickly too eg oavlov's dogs.

* Lessons can be learnt just by thinking. Eg a soldier who has never been in combat tensing up when they come to a ravine because they have been told such places are good for ambushes.

* If you have a feeling about a course of action that you don't know the source of and are then right, you will label this intuition.

* Author and Kline / Klein disagree on validity of intuition partly because rather studied different experts. Kline happened to study lots of people with genuine expertise eg chess players and firefighters.

* They both agreed that you can't trust someone's evaluation of the validity of their own intuitions.
Two basic conditions for acquiring a skill, - you need an environment that is sufficiently regular to be predictable and an opportunity to learnt the regularities through prolonged practice.

* Intuition that is reliable builds up through repeated practice and feedback on performance. Eg steering a car can be learnt quickly because many of the possible bends come up frequently and the movement of a car in response to steering is immediate. In contrast it's harder to learn to steer a huge boat because the boat change in movement is delayed. Psychotherapist can learn good intuition about how to talk to a patient to elicit desired reactions eg claiming them down or getting them to reflect on something. But it's way harder to learn intuition about how a patient will do long term because the reaction to any action of the psychotherapist are so delayed.

* Better to judge the intuition of a person on a decision by looking at whether the environment is regular and if the person has had a chance to learn its regularities rather than the decision itself. Perfectly possible for wrong intuition to seem plausible especially to a non expert.

* Intuition is learnt by experience being fed to the associative machinery of earlier in the book.
Confidence in judgement is not really affected by accuracy or rather other factors like cognitive ease and coherence affect confidence a lot.

* When you look back on a successful project it can often seem to have taken way too long. But it's success makes it seem like the solution was the only possible one and so it was a firegone conclusion you should have seen way faster.

* Even through author and Kline ended up agreeing on a lot of their work, their attitudes and beliefs (emotional) didn't change much. Kline still enjoyed stories about algorithms producing absurd results and saw bad judgements by experts as rare. Author still enjoyed stories about arrogant experts getting it totally wrong and assumed that a bad algorithm for an opportunity for improvement.

* Author ran team to produce textbook and curriculum about decision making and judgement for Israeli schools. After they had been working a while got everyone to secretly estimate how long left to go. Estimates averaged to 2 years with no big outliers. Asked member of team who knew a lot about teams making textbooks and curricula who had voted to say how the teams he knew about had done. Person said 40% had failed and had all taken between 7 to 10 years. Asked if this team was better or worse than outhouse teams. He said slight worse. They got a bit down and then carried right on. The task was finished after 8 years. Lessons. Planning mostly assumes best case scenario. There are two ways to forecast things - inside view and outside view. Outside was way more accurate. Base rates were ignored. It is common to keep going with something you rationally know is futile.

* Outside view is almost always rejected in Cavour of inside view. Pallid statistics way less appealing than causal narrative.

* Reference class forecasting is sometimes used in large building projects to produce more accurate estimates of cost and time. A database of projects is kept and referred to. A project is fitted to a reference class of similar projects and that is used as the base rate of time and money. Stats might be average cost or something specific like cost per mile of railway. The optimistic estimate can be used but only to adjust from Base rates.

* Optimists carry on with unsuccessful things longer than others. They also tend to have a greater effect on the world than others.

* People employed by s company tend to do better financially than people who are self employed.

* Small business owners tend to rate the odds of success at twice that is the real base rate odds of success.

* People tend to see themselves as above average in most measurements.

* Or modified people take more unwise risks.

* CEOs who get awards tend to do worse than CEOs who don't. They spend more time writing books and sitting on other boards. Is this because they become over confident or because they get pulled into more outside activities because of their fame.

* Entrepreneurs often feel they will succeed where others will fail eg the young hotel owner couple who bought a motel cheap because six or seven other people had failed to make it work.

* All this entrepreneur stuff is not just wishful thinking of optimists. What you see is all their is: Planning fallacy and neglect ion of base rates, focus on what we can and want to do and neglect  to take into account skills of others, focus on causal skill and neglect role of luck  (illusion of control).

* Why do movie studios release big budget films on the same day as other big budget movies from other studios? Because of competition neglect. They think about their own film and marketing department and think they are good and don't think about the other companies at all.

* Overconfidence is rewarded in some professions. No one wants an uncertain CFO or Doctor.
People who confidently take risks are often not making bold bets but instead are oblivious to the risk.

* Overconfidence is an advantage for people who face frequent setbacks and few successes eg researchers and salesmen. But this is only helpful if they are on the right path.

* To avoid overconfidence, do a pre mortem. Just before finalizing a big decision, Imagine the big decision happened and you are a year in and it's a disaster. Gather the working team together and get them to write down an account of what went wrong. Public doubts are slowly groupthink surpressed as a decision a certain way seems to be a forgone conclusion and this lets people voice them and also forces everyone to focus on the bad part of the plan.

* Economists assume economic actors are rational, selfish and have unchanging tastes (econs vs the phychology view of a more realistic human).

* Decision theory's basic experiment - like geneticist's fruit fly - is simple gambles eg how much would you bet on winning $300 if...

* Basic theory of decision theory field is expected utility which is the foundation of the rational agent mode and is the most important theory in the social sciences. This was not intended as a physiological model but was a logic based on choice. If you prefer apples to bananas you will prefer a 10% chance to win an apple to a 10% chance to win a banana. This theory describes both how decisions should be made and how econs make choices.

* German psychologist and mystic Gustav vekner started and name field of psycho physics in 1800s. This is the field of understanding the relationship between physical changes and mental changes. His findings were that changes in intensity for both physical and mental were logarithmic. Eg if change I. Intensity if light from 10 to 100 makes a change in mental intensity of 4 units then a change from 100 to 1000 also changes mental intensity by 4 units.

* Bounoiilli reasoned in the same way about money but 100 years earlier. If a rich person gets a 30% raise they will feel as excited about it as a poor person who gets a 30% raise. But a poor person will feel more excited about a 100$ raise than a rich person. So utility of money is inversely proportional to wealth.

* Bernoulli said this changes if you introduce risk. A person will probably prefer to get 46$ for sure than a 50% chance of 100$. This is because people don't evaluate the average dollar value of the possible outcomes weighted by chance. Instead they evaluate the average if the utilities of these outcomes each weighted by its probability.
Eg:

* wealth (millions)  1   2    3  4   5   6   7   8   9   10
utility (units)        10 30 48 60 70 78 84 90 96 100

* Gamble of 50% chance of 1 or 7 million vs sure thing of 4 million.  People choose 4 million sure thing.  Expected value of both is the same (4 million), so people should be indifferent.  But psychological utilities are:

10 + 84 / 2 = 47
60 / 1 = 60

* This explains why poor people buy insurance from rich people.

* Bernoulli theory all very well but only part for story. Happiness is also affected by change. Jack has 1m Jill has 9m. If they both the next day have 5m, jack will be happy and Jill will be sad even though 5m has a single utility number.

* Another example. Which do you choose. Get 900 for sure or 90% chance of 1000. People choose the 900. Which do you choose. Lose 900 for sure or 90% chance of losing 1000. People choose the gamble. This is because the negative value of losing 900 is much more than 90% of the negative value of losing 1000. People are very averse to sure losses. People become risk averse when all their options are bad.

* In expected utility theory (Bernoulli) they would both be expected to make the same decision in both cases (doesn't matter which they are equally valuable) because the expected utility of each gain is equal to the expected utility of each loss

* Bernoulli's utility theory explains risk aversion in poor people But it doesn't explain risk seeking in rich people. Prospect theory explains both. Bernoulli assumes that the sadness at losing money is equal to the opposite happiness at gaining the same money. This is wrong under prospect theory.

* Bernoulli does incorporate changes in wealth but only in the conparison of one utility to another. Eg choice between 1m and 2m is described by the choice between the utility of 10 units to 30 units. Eg utility of a gain is described by comparing utility of two states of wealth - utility of getting an extra 500 when your wealth is 1m is the difference in utilities of 1m and 1000500. The utility of losing 500 when your wealth is 1000500 is the opposite.

* Prospect theory. Reference point matters. Smaller changes have way less effect on us than large changes. And we are more sensitive to losses than gains.

* We are more sensitive to losses because organisms that treat threats as more urgent than gains have a better opportunity to survive and reproduce.

* Graph of prospect theory. S shaped which represents diminishing sensitivity to gains and losses. Also not symmetrical which represents the greater sensitivity to losses.  This graph explains why people are risk averse in mixed gambles where gains and losses are possible AND AT THE SAME TIME risk seeking when only losses are possible.  It's because each example covers a different area of the graph.  In the mixed case, the possible loss looms twice as large as the possible gain, as you can see by comparing the slopes of the value function for losses and gains. In the bad case, the bending of the value curve (diminishing sensitivity) causes risk seeking.

* I'm not sure I am subject to prospect theory.  Yet another example: 50% chance of either losing 100 or gaining 150. Do you take it? I choose the gamble.  Apparently most people don't.  Author says risk/loss coefficient is how much gain you need to stand to get to assume a risk.  Maybe my coefficient is just lower than the average person's.  To me though the theory seems to miss out another reference point - how much money you have.  They try and explain this by allowing for a changing risk coefficient based on the size of the gamble but this doesn't really seem to be part of the theory.

* Theory indices blindness - ignoring counter examples when you have a theory in mind.
Prospect theory has the assumption that the reference point (usually status quo) has value of 0.  This is not always true. eg a gamble has a high chance of a win eg 90 percent chance to win 1000 or get nothing probably makes the reference point 1000 not no win (0). The real value of winning nothing is a strong negative value, not 0, because of the emotion of disappointment.

* There are (more complex) theories that incorporate disappointment, but they have not caught on like prospect theory because, though people who lose the above gamble will feel disappointment, they will still take the gamble so the theory doesn't predict different behavior.  In contrast, prospect theory does correctly predict different outcomes in quite a few cases than utility theory.  Scientists will only take on extra complexity of a theory if it is justified by significant extra usefulness.

* The missing tool in the stuff about prospect theory is how to figure out the reference point. Without this info it's much harder to see applications of the theory. Eg in the bottle of wine endowment example - person will buy a bottle for max 35 but only sell it for min 100 - reference point is owning or not owning the bottle. But in most examples the reference point is based on money. In most money examples the reference point is current money but in recent example of the 90% chance bet it is the hoped for win money.

* Endowment effect. You care more about losing something you have than gaining something you don't have eg bottle of wine.

* We are fine with some transactions and the loss of one thing weighs no heavier than the gain of the their. eg exchanging money for shoes at a shoe shop. This is because we have two categories - things for use like wine and things for exchange like shoes for a shoe shop owner.

* Brain responds to expressions very fast, but it responds to angry or scared face a couple of hundred ms faster than happy face.

* Bad overrides good in a broad range of scenarios.

* Threats are prioritized over opportunities.

* Reference point changes. Eg you want to earn a bit more money per year, but once you do, you want to earn another bit more.

* People who set goals want to avoid failing at them more than they want to succeed at them. If a cabbie has a target income per year, they will focus much more on their daily earnings goal to achieve their annual goal. This means that on rainy days when it's easy to earn money they will drive less and on sunny days they will drive more.
Concessions in negotiations are felt more than gains. This means that a party must give a lot to keep a little.

* Loss aversion means reforms are less effective and more expensive than they might be. If people who lose stuff in the reforms have political power, they will make the reformers overpay for the losses.
Loss aversion is the gravitational force that holds our life stable - job, neighborhood, friends - near the reference point.

* Telephone survey. Hardware store raises price of shovels from 15 to 20 after a storm. Is this fair or unfair. 82% said unfair. Not fair to use market forces to make people lose. But for me the crucial thing is - is it exploitative of people's need to buy a shovel or does it just increase the amount they must pay to a still affordable point? I think people thinking it's the former makes them think it's unfair.

* It's easier to pass reforms where the pie expands rather than contracts. People fight less fervently against others gaining than themselves losing.

* Pleasure centers activate when reprimanding a stranger who has been unfair to another stranger. This is altruistic punishment and it's probably important to society
We remember the painful was of an experience by a combo of the highest pain we experienced and the end of the experience. So if there was a slow lessening of pain we will remember it as better than if there was a lower pain through and up to the end.

* We learn from our memories, not our experiences.

* Immerse hand in painfully cold water or 60s. Next immerse other hand in paibfully cold water for 60s and then another 30s with temp raised very slightly. Now choose which experience to repeat. 80 percent chose to repeat second experience. Decision utility dominates experienced utility. This is another example of sets being evaluated by their averages or norms or prototypes not sums, just like adding dishes to 24 dishes lowers their total value because some are broken and Jane being judged as more likely to be a feminist bank teller than a bank teller.

* Experience of event is really a sum of moments of pain. But memory chooses salient moments - peak pain and end and uses them as prototypes.

* People's goals affect their happiness with their life reasonably significantly. People who wanted to be rwell off and became well off ended up happier than people who wanted to but didn't. The happiness with life of People who didn't particularly want to become well off wasn't strongly affected by their eventual well offness.

* People over estimate the pleasure of a purchase like a car over a long time and under estimate the pleasure of a repeated pleasure like a weekly poker game. You stop focusing on the car but you continue to give regular attention to the poker game.

* He says that the well being of people who live in Cali is no greater than those who live in a very cold place. He explains this by saying people stop thinking about things once they are no longer new. But that is not my experience - every time I came out into a sunny day in New York and got to walk to work I enjoyed it. Maybe it was the walk rather than the weather and so I focused on it regularly a la the poker game. But if so what makes one often present thing a focal point like the walk or poker game And another thing like the car something you stop noticing?
